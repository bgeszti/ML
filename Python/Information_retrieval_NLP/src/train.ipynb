{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D8usSW9Bwv4h"
   },
   "source": [
    "# AIR - Exercise in Google Colab\n",
    "\n",
    "## Colab Preparation\n",
    "\n",
    "Open via google drive -> right click: open with Colab\n",
    "\n",
    "**Get a GPU**\n",
    "\n",
    "Toolbar -> Runtime -> Change Runtime Type -> GPU\n",
    "\n",
    "**Mount Google Drive**\n",
    "\n",
    "* Download data and clone your github repo to your Google Drive folder\n",
    "* Use Google Drive as connection between Github and Colab (Could also use direct github access, but re-submitting credentials might be annoying)\n",
    "* Commit to Github locally from the synced drive\n",
    "\n",
    "**Keep Alive**\n",
    "\n",
    "When training google colab tends to kick you out, This might help: https://medium.com/@shivamrawat_756/how-to-prevent-google-colab-from-disconnecting-717b88a128c0\n",
    "\n",
    "**Get Started**\n",
    "\n",
    "Run the following script to mount google drive and install needed python packages. Pytorch comes pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Sfiw_6jZ0uWa",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 856
    },
    "outputId": "6b3f80b6-55ce-4f1a-90dd-1c58975028f8"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!pip install allennlp==0.9.0\n",
    "#!pip install allennlp\n",
    "#!pip uninstall allennlp\n",
    "#!pip install allennlp==1.0.0rc6"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n",
      "Collecting allennlp==0.9.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n",
      "\u001B[K     |████████████████████████████████| 7.6MB 3.0MB/s \n",
      "\u001B[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (0.5.3)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (1.4.1)\n",
      "Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (3.6.4)\n",
      "Collecting parsimonious>=0.8.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n",
      "\u001B[K     |████████████████████████████████| 51kB 7.1MB/s \n",
      "\u001B[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (3.2.5)\n",
      "Collecting spacy<2.2,>=2.1.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n",
      "\u001B[K     |████████████████████████████████| 30.9MB 96kB/s \n",
      "\u001B[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/86/70/ed1ba808a87d896b9f4d25400dda54e089ca7a97e87cee620b3744997c89/jsonnet-0.16.0.tar.gz (256kB)\n",
      "\u001B[K     |████████████████████████████████| 266kB 31.8MB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (4.41.1)\n",
      "Collecting ftfy\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n",
      "\u001B[K     |████████████████████████████████| 61kB 6.1MB/s \n",
      "\u001B[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (1.5.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (1.18.5)\n",
      "Collecting tensorboardX>=1.2\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n",
      "\u001B[K     |████████████████████████████████| 204kB 36.6MB/s \n",
      "\u001B[?25hCollecting numpydoc>=0.8.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/3a/43/2402fd1f63992a52f88e3b169d59674617013bf7f1ad0cd7d842eafd0c58/numpydoc-1.0.0-py3-none-any.whl (47kB)\n",
      "\u001B[K     |████████████████████████████████| 51kB 6.9MB/s \n",
      "\u001B[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (2.10.0)\n",
      "Collecting overrides\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Collecting unidecode\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
      "\u001B[K     |████████████████████████████████| 245kB 35.6MB/s \n",
      "\u001B[?25hCollecting word2number>=1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp==0.9.0) (1.14.9)\n",
      "Collecting flask-cors>=3.0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n",
      "Collecting pytorch-transformers==1.1.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n",
      "\u001B[K     |████████████████████████████████| 163kB 36.7MB/s \n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "IUVVDw1m2sed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1593268331196,
     "user_tz": -120,
     "elapsed": 523,
     "user": {
      "displayName": "András Dörömbözi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9C95QLes3NjxA1wPZhv1biT4fTzgpqiKXGqqz_Q=s64",
      "userId": "17605991551541111367"
     }
    },
    "outputId": "d4904e51-5ff6-40d1-9356-aae41957f3a3"
   },
   "source": [
    "import torch\n",
    "\n",
    "print(\"Version:\",torch.__version__)\n",
    "print(\"Has GPU:\",torch.cuda.is_available()) # check that 1 gpu is available\n",
    "print(\"Random tensor:\",torch.rand(10,device=\"cuda\")) # check that pytorch works "
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Version: 1.5.1+cu101\n",
      "Has GPU: True\n",
      "Random tensor: tensor([0.0223, 0.1034, 0.0115, 0.8287, 0.3820, 0.7451, 0.8807, 0.4712, 0.6000,\n",
      "        0.0270], device='cuda:0')\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rgz34ioDO9ct",
    "colab_type": "text"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fvQMmxs0x_x8"
   },
   "source": [
    "# Main.py Replacement\n",
    "\n",
    "-> add your code here\n",
    "\n",
    "- Replace *air_test* with your google drive location in the sys.path.append()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Y_IEUP_2-099",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "outputId": "d435b4de-fefa-430b-bef4-610b656d075f"
   },
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/air/src')\n",
    "\n",
    "from allennlp.common import Params, Tqdm\n",
    "from allennlp.common.util import prepare_environment\n",
    "prepare_environment(Params({})) # sets the seeds to be fixed\n",
    "\n",
    "import torch\n",
    "import allennlp.data\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.data import *\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.text_field_embedders import BasicTextFieldEmbedder\n",
    "from allennlp.data.tokenizers.word_splitter import JustSpacesWordSplitter\n",
    "\n",
    "\n",
    "from data_loading import *\n",
    "from model_knrm import *\n",
    "from model_conv_knrm import *\n",
    "from model_match_pyramid import *\n",
    "\n",
    "# change paths to your data directory\n",
    "# executing path is /content -> so change paths accordingly\n",
    "pathPrefix = \"/content/drive/My Drive\"\n",
    "config = {\n",
    "    \"vocab_directory\": pathPrefix + \"/data/allen_vocab_lower_10\",\n",
    "    \"pre_trained_embedding\": pathPrefix + \"/data/glove.42B.300d.txt\",\n",
    "    \"model\": \"knrm\",\n",
    "    #\"model\": \"conv_knrm\",\n",
    "    \"train_data\": pathPrefix + \"/data/triples.train.tsv\",\n",
    "    \"validation_data\": pathPrefix + \"/data/msmarco_tuples.validation.tsv\",\n",
    "    \"test_data\": pathPrefix  + \"/data/msmarco_tuples.test.tsv\",\n",
    "}\n",
    "\n",
    "#\n",
    "# data loading\n",
    "#\n",
    "\n",
    "vocab = Vocabulary.from_files(config[\"vocab_directory\"])\n",
    "tokens_embedder = Embedding.from_params(vocab, Params({\"pretrained_file\": config[\"pre_trained_embedding\"],\n",
    "                                                      \"embedding_dim\": 300,\n",
    "                                                      \"trainable\": True,\n",
    "                                                      \"padding_index\":0}))\n",
    "\n",
    "word_embedder = BasicTextFieldEmbedder({\"tokens\": tokens_embedder})\n",
    "\n",
    "# recommended default params for the models (but you may change them if you want)\n",
    "if config[\"model\"] == \"knrm\":\n",
    "    model = KNRM(word_embedder, n_kernels=11)\n",
    "elif config[\"model\"] == \"conv_knrm\":\n",
    "    model = Conv_KNRM(word_embedder, n_grams=3, n_kernels=11, conv_out_dim=128)\n",
    "elif config[\"model\"] == \"match_pyramid\":\n",
    "    model = MatchPyramid(word_embedder, conv_output_size=[16,16,16,16,16], conv_kernel_size=[[3,3],[3,3],[3,3],[3,3],[3,3]], adaptive_pooling_size=[[36,90],[18,60],[9,30],[6,20],[3,10]])\n",
    "\n",
    "\n",
    "# todo optimizer, loss \n",
    "\n",
    "print('Model',config[\"model\"],'total parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print('Network:', model)\n",
    "\n",
    "#\n",
    "# train\n",
    "#\n",
    "iterBatchSize = 64\n",
    "_triple_loader = IrTripleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30,tokenizer = WordTokenizer(word_splitter=JustSpacesWordSplitter())) # already spacy tokenized, so that it is faster \n",
    "\n",
    "_iterator = BucketIterator(batch_size=iterBatchSize,\n",
    "                           sorting_keys=[(\"doc_pos_tokens\", \"num_tokens\"), (\"doc_neg_tokens\", \"num_tokens\")])\n",
    "\n",
    "_iterator.index_with(vocab)\n",
    "\n",
    "#Create a folder which will store the model state, and the results: model name + current timestamp without seconds\n",
    "from datetime import datetime\n",
    "import os \n",
    "\n",
    "dt_string = datetime.now().strftime(\"%d-%m-%Y-%H_%M\")\n",
    "newFolder = str(config[\"model\"]) + \"_\" + dt_string + '/'\n",
    "resultFolder = pathPrefix + '/air_results/' + newFolder\n",
    "os.mkdir(resultFolder)\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "1157587it [01:25, 12053.39it/s]"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab_type": "code",
    "id": "UXeFEfQemSVE",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 246
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1593268597362,
     "user_tz": -120,
     "elapsed": 697,
     "user": {
      "displayName": "András Dörömbözi",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gh9C95QLes3NjxA1wPZhv1biT4fTzgpqiKXGqqz_Q=s64",
      "userId": "17605991551541111367"
     }
    },
    "outputId": "ff1346a6-76ec-40c2-c2ea-775c4c81de23"
   },
   "source": [
    "from helper_Methods import *\n",
    "from core_metrics import *\n",
    "from allennlp.nn.util import *\n",
    "\n",
    "#to hide \"elementwise-mean is deprecated\" warning\n",
    "loss_functionCriterion = torch.nn.MarginRankingLoss (margin=1, reduction= 'mean')\n",
    "#Initialize ADAM optimizer\n",
    "optimAdam = torch.optim.Adam(model.parameters(),lr=0.0001)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "trainBatchSize = iterBatchSize\n",
    "#initiailize early stopping: We will decide based on the MRR that the model is better or not\n",
    "valMetricRes = {}\n",
    "valMetricRes[\"MRR@10\"] = 0\n",
    "earlyStop = False\n",
    "\n",
    "#creat lists to monitor loss\n",
    "train_losses = {}\n",
    "perf_monitor = PerformanceMonitor()\n",
    "monitorModel = \"train-\" + str(config[\"model\"])\n",
    "\n",
    "#creat lists to store the validation results for vizualization\n",
    "allValResults = {}\n",
    "\n",
    "iterCounter = 0\n",
    "for epoch in range(2):\n",
    "    #if early stopping has been triggered during validation, exit from the epoch\n",
    "    if earlyStop is True:\n",
    "        break\n",
    "\n",
    "\n",
    "    perf_monitor.start_block(monitorModel)\n",
    "    perf_start_inst = 0\n",
    "    # prep model for training\n",
    "    model.train()\n",
    "    # Creating a label tensor filled with ones --> will be needed for marginranking loss\n",
    "    # should be initialized in each outer loop, since in the last loop in the inner cycle the size of the tensor\n",
    "    # will probably change\n",
    "    label = torch.ones(trainBatchSize).cuda()\n",
    "    batchCounter = 0\n",
    "    # Train loop\n",
    "    for batch in Tqdm.tqdm(_iterator(_triple_loader.read(config[\"train_data\"]), num_epochs=1)):\n",
    "        iterCounter += 1\n",
    "        batch = move_to_device(batch, 0)\n",
    "        #batch  = Parameter(batch).to(device)\n",
    "        batchCounter += 1\n",
    "        model.train()\n",
    "        # todo train loop\n",
    "        #in the beggining of each train loop, clean the optimizer (zero_grad() method)\n",
    "        optimAdam.zero_grad()\n",
    "        #retrieve the current batch size --> The iterators do not guarantee a fixed batch size\n",
    "        # (the last one will probably be smaller) --> so we will retrieve the number of tokens from e.g. the query\n",
    "        currentBatchSize = batch[\"query_tokens\"][\"tokens\"].shape[0]\n",
    "        # for the batch size, th\n",
    "\n",
    "        # based on the slides, the model will be trained with triplets:\n",
    "        # Triple: 1 query, 1 relevant + 1 non relevant document\n",
    "        # where the relevant documents are: batch['doc_pos_tokens']\n",
    "        # and the non relevants: batch['doc_neg_tokens']\n",
    "        # the goal is to maximize the margin between these documents, that's why we apply the margin loss\n",
    "        #we will apply the 2 'forward' pass:\n",
    "        # so we will call model.forward() twice, once for the relevant, and second time for the non-relevant doc\n",
    "        relevantDocsOutPut = model.forward(batch[\"query_tokens\"],batch[\"doc_pos_tokens\"]).cuda()\n",
    "\n",
    "        nonRelevantDocsOutPut = model.forward(batch[\"query_tokens\"], batch[\"doc_neg_tokens\"]).cuda()\n",
    "\n",
    "        #If the last batch is smallert than the other ones, we need a smaller label tensors, else we will get\n",
    "        # dimensionality mismatch exceptions\n",
    "        if currentBatchSize != trainBatchSize:\n",
    "            #reduce the label tensor size to the size of the last (or current batch)\n",
    "            label = torch.ones(currentBatchSize).cuda()\n",
    "\n",
    "        # calculate the loss --> it's important to place the relevant docs as the first parameter, due to the usage\n",
    "        # of the loss function: If y (the label) = 1 then it assumed the first input should be ranked higher\n",
    "        # (have a larger value) than the second input,  # and vice-versa for y = -1\n",
    "        loss = loss_functionCriterion(relevantDocsOutPut, nonRelevantDocsOutPut, label).cuda(torch.device(\"cuda\")) \n",
    "\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # perform a single optimization step (parameter update)\n",
    "        # optimizer step() will calculate the loss\n",
    "        optimAdam.step()\n",
    "        # record training loss\n",
    "        #train_losses.append(loss.item())\n",
    "        \n",
    "        train_losses.setdefault(iterCounter, loss.item())\n",
    "\n",
    "        label = torch.ones(trainBatchSize).cuda()\n",
    "\n",
    "        # If the  batchCOunter divisor with 7500 returns 0: evaluate the model against the validation set        \n",
    "        if batchCounter % 7500 == 0:        \n",
    "            _tuple_loader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30) # not spacy tokenized already (default is spacy)\n",
    "            _iterValidation = BucketIterator(batch_size=128,\n",
    "                                      sorting_keys=[(\"doc_tokens\", \"num_tokens\"), (\"query_tokens\", \"num_tokens\")])\n",
    "            _iterValidation.index_with(vocab)\n",
    "            # Validation loop\n",
    "            # Set the model to evaluation\n",
    "            validationResults = {}\n",
    "            model.eval()\n",
    "            \n",
    "            perf_monitor.stop_block(monitorModel,(batchCounter - perf_start_inst) * currentBatchSize)\n",
    "            perf_start_inst = batchCounter\n",
    "            perfVal = \"validation-\" + str(config[\"model\"])\n",
    "            perf_monitor.start_block(perfVal)\n",
    "\n",
    "            totalSize = 0\n",
    "            for batchVal in Tqdm.tqdm(_iterValidation(_tuple_loader.read(config[\"validation_data\"]), num_epochs=1)):\n",
    "                totalSize += currentBatchSize\n",
    "                batchVal = move_to_device(batchVal, 0)\n",
    "\n",
    "                currentBatchSize = batchVal[\"query_tokens\"][\"tokens\"].shape[0]\n",
    "                output = model.forward(batchVal[\"query_tokens\"], batchVal[\"doc_tokens\"]).cuda()            \n",
    "                for q,d,o in zip(batchVal[\"query_id\"].tolist(),batchVal[\"doc_id\"].tolist(), output.tolist()):\n",
    "                    queryID = str(q)\n",
    "                    docID = str(d)\n",
    "                    if queryID not in validationResults:\n",
    "                      validationResults[queryID] = []\n",
    "                   # print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "                   # print(\"qr: \" + str(q) + \" docid: \" + str(d) + \" output: \" + str(o))\n",
    "                    validationResults[queryID].append((docID, float(o)))\n",
    "            \n",
    "            \n",
    "            \n",
    "            perf_monitor.stop_block(perfVal,totalSize)\n",
    "            perf_monitor.start_block(monitorModel)\n",
    "            perf_monitor.print_summary()\n",
    "            ranked_validadtionResults = unrolled_to_ranked_result(validationResults)\n",
    "\n",
    "            #Create the validation results to ranked results:\n",
    "            metrics = calculate_metrics_plain(ranked_validadtionResults, load_qrels(pathPrefix + \"/data/msmarco_qrels.txt\"),\n",
    "                                              binarization_point=1)\n",
    "            \n",
    "            if iterCounter not in allValResults:\n",
    "              allValResults[iterCounter] = {}\n",
    "            #Store the results\n",
    "            allValResults[iterCounter] = metrics\n",
    "            #allValResults.append(metrics)\n",
    "\n",
    "            print('#####################')\n",
    "            for metric in metrics:\n",
    "                print('{}: {}'.format(metric, metrics[metric]))\n",
    "            print('#####################')\n",
    "            print('==========================')\n",
    "            #print(metrics.keys())\n",
    "            \n",
    "\n",
    "            \n",
    "            #If the current MRR is better than the previous one, overwrite it, and save the model maybe?\n",
    "            print(\"actual MRR: \" + str(metrics[\"MRR@10\"]))\n",
    "            print(\"reference MRR : \" + str(valMetricRes[\"MRR@10\"]))\n",
    "            print('==========================')\n",
    "\n",
    "            if(metrics[\"MRR@10\"] > valMetricRes[\"MRR@10\"]):\n",
    "                valMetricRes[\"MRR@10\"] = metrics[\"MRR@10\"]\n",
    "                #Save the model\n",
    "                torch.save(model.state_dict(), resultFolder + str(config[\"model\"]) + \"_best-model.pytorch-state-dict\")\n",
    "                modelToSave = model.state_dict()\n",
    "\n",
    "            elif(metrics[\"MRR@10\"] <= valMetricRes[\"MRR@10\"]):\n",
    "                earlyStop = True\n",
    "                print(\"Early stopping initiated\")\n",
    "                break\n",
    "          \n",
    "\n",
    "    perf_monitor.stop_block(monitorModel,batchCounter - perf_start_inst)\n"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-8-a1ea039a04e6>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     43\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mbatch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mTqdm\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtqdm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_iterator\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_triple_loader\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m\"train_data\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m         \u001B[0miterCounter\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 45\u001B[0;31m         \u001B[0mbatch\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmove_to_device\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbatch\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     46\u001B[0m         \u001B[0;31m#batch  = Parameter(batch).to(device)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     47\u001B[0m         \u001B[0mbatchCounter\u001B[0m \u001B[0;34m+=\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'move_to_device' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "5ZDd4I3rv2Cv",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Load the model from the best state\n",
    "\n",
    "_tuple_loader = IrLabeledTupleDatasetReader(lazy=True, max_doc_length=180, max_query_length=30) # not spacy tokenized already (default is spacy)\n",
    "_iterator = BucketIterator(batch_size=128,\n",
    "                           sorting_keys=[(\"doc_tokens\", \"num_tokens\"), (\"query_tokens\", \"num_tokens\")])\n",
    "_iterator.index_with(vocab)\n",
    "\n",
    "#create an evaluation model, then load back the model state with the best validation result\n",
    "if config[\"model\"] == \"knrm\":\n",
    "    modelToEval = KNRM(word_embedder, n_kernels=11)\n",
    "elif config[\"model\"] == \"conv_knrm\":\n",
    "    modelToEval = Conv_KNRM(word_embedder, n_grams=3, n_kernels=11, conv_out_dim=128)\n",
    "elif config[\"model\"] == \"match_pyramid\":\n",
    "    modelToEval = MatchPyramid(word_embedder, conv_output_size=[16,16,16,16,16], conv_kernel_size=[[3,3],[3,3],[3,3],[3,3],[3,3]], adaptive_pooling_size=[[36,90],[18,60],[9,30],[6,20],[3,10]])\n",
    "\n",
    "modelToEval.load_state_dict(modelToSave)\n",
    "modelToEval = modelToEval.to(device)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "omUWZPpWv2Cx",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Evaluate the results on MSMARCO\n",
    "perfEvalMsMarco = \"test-msmarcoPerformance\" + str(config[\"model\"])\n",
    "perf_monitor.start_block(perfEvalMsMarco)\n",
    "modelToEval.eval()\n",
    "msMarcoresults = {}\n",
    "for batch in Tqdm.tqdm(_iterator(_tuple_loader.read(config[\"test_data\"]), num_epochs=1)):\n",
    "    # todo test loop\n",
    "    batch = move_to_device(batch, 0)\n",
    "    #output = model.forward(batch[\"query_tokens\"], batch[\"doc_tokens\"]).cuda()\n",
    "    output = modelToEval.forward(batch[\"query_tokens\"], batch[\"doc_tokens\"]).cuda()\n",
    "    for q,d,o in zip(batch[\"query_id\"].tolist(),batch[\"doc_id\"].tolist(), output.tolist()):\n",
    "      queryID = str(q)\n",
    "      docID = str(d)\n",
    "      if queryID not in msMarcoresults:\n",
    "        msMarcoresults[queryID] = []\n",
    "      # print('+++++++++++++++++++++++++++++++++++++++++')\n",
    "      # print(\"qr: \" + str(q) + \" docid: \" + str(d) + \" output: \" + str(o))\n",
    "      msMarcoresults[queryID].append((docID, float(o)))\n",
    "\n",
    "perf_monitor.stop_block(perfEvalMsMarco)\n",
    "    #pass\n",
    "#Set the results to appropriate format\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "HvRjbkbnv2Cz",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "msMarcoRankedResults = unrolled_to_ranked_result(msMarcoresults)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "dU18cBNZv2C1",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "msMarcoevaluationResults = calculate_metrics_plain(msMarcoRankedResults, load_qrels(pathPrefix + \"/data/msmarco_qrels.txt\"), binarization_point=1)\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "eUW6EcXpv2C2",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print('#####################')\n",
    "print('eval on ms marco test set')\n",
    "for metric in msMarcoevaluationResults:\n",
    "  print('{}: {}'.format(metric, msMarcoevaluationResults[metric]))\n",
    "print('#####################')\n",
    "print('==========================')\n",
    "#print(metrics.keys())\n",
    "print('==========================')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "IoN3PYp5v2C4",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Evaluate the results on the FIRA dataset\n",
    "perfEvalFira = \"test-firaPerformance\" + str(config[\"model\"])\n",
    "perf_monitor.start_block(perfEvalFira)\n",
    "\n",
    "modelToEval.eval()\n",
    "firaResults = {}\n",
    "for batch in Tqdm.tqdm(_iterator(_tuple_loader.read(pathPrefix + \"/data/fira_numsnippets_test_tuples.tsv\"), num_epochs=1)):\n",
    "    # todo test loop\n",
    "    batch = move_to_device(batch, 0)\n",
    "    #output = model.forward(batch[\"query_tokens\"], batch[\"doc_tokens\"]).cuda()\n",
    "    output = modelToEval.forward(batch[\"query_tokens\"], batch[\"doc_tokens\"]).cuda()\n",
    "    for q,d,o in zip(batch[\"query_id\"].tolist(),batch[\"doc_id\"].tolist(), output.tolist()):\n",
    "      queryID = str(q)\n",
    "      docID = str(d)\n",
    "      if queryID not in firaResults:\n",
    "        firaResults[queryID] = []\n",
    " \n",
    "      firaResults[queryID].append((docID, float(o)))\n",
    "\n",
    "perf_monitor.stop_block(perfEvalFira)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "cMFPby7Lv2C6",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#Set the results to appropriate format\n",
    "firaRankedResults = unrolled_to_ranked_result(firaResults)\n",
    " \n",
    "firaevaluationResults = calculate_metrics_plain(firaRankedResults, load_qrels(pathPrefix + \"/data/fira_numsnippets_qrels.txt\"), binarization_point=1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "8o2lv8bSv2C9",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "print('#####################')\n",
    "print('eval on fira test set')\n",
    "for metric in firaevaluationResults:\n",
    "  print('{}: {}'.format(metric, firaevaluationResults[metric]))\n",
    "print('#####################')\n",
    "print('==========================')\n",
    "#print(metrics.keys())\n",
    "print('==========================')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VaxsSslaDxHe",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "#!pip install pandas\n",
    "import pandas as pd\n",
    "res=pd.concat({k: pd.DataFrame(v) for k, v in firaResults.items()}, axis=0)\n",
    "result=pd.DataFrame(res.values,index=res.index.droplevel(1), columns=['document', 'relavance']).to_csv(resultFolder + str(config[\"model\"]) + '_firaRelevances.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "zaDMUqMKv2C-",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "\n",
    "# to pandas\n",
    "\n",
    "dfMarco = pd.DataFrame.from_dict(msMarcoevaluationResults, orient=\"index\").to_csv(resultFolder + str(config[\"model\"]) + '_msmarco.csv')\n",
    "\n",
    "fr = pd.DataFrame.from_dict(firaevaluationResults, orient=\"index\").to_csv(resultFolder + str(config[\"model\"]) + '_fira.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XqJEKFofKArB",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "allValidationResultsPd = pd.DataFrame.from_dict({(i): allValResults[i] for i in allValResults.keys() },\n",
    "                       orient='index')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bw1KXRWEHnlQ",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "allValidationResultsPd.to_csv(resultFolder + str(config[\"model\"]) + '_validationResults.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Aa9vgSDXIWvy",
    "colab": {}
   },
   "source": [
    "tl = pd.DataFrame.from_dict(train_losses, orient=\"index\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ci4pm2fGKFSG",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "tl.to_csv(resultFolder + str(config[\"model\"]) + '_lossResults.csv')"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}